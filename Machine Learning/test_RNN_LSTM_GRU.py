import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.datasets import AG_NEWS
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
import time
import os

# ë°ì´í„° ì „ì²˜ë¦¬
tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter): # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”(tokenize)í•˜ì—¬ í•œ ì¤„ì”© ë„˜ê²¨ì£¼ëŠ” ì œë„ˆë ˆì´í„°ë¥¼ ë§Œë“œëŠ” ê²ƒ
    for label, line in data_iter: # ì˜ˆ: (3, "Breaking news: stock prices are rising...")
        yield tokenizer(line) # line (í…ìŠ¤íŠ¸ ë¬¸ìì—´)ì„ tokenizerë¡œ ë¶„í•´í•´ì„œ ë‹¨ì–´ ëª©ë¡ì„ ìƒì„±

train_iter = AG_NEWS(split='train') # torchtext.datasets.AG_NEWSì—ì„œ í›ˆë ¨ ë°ì´í„°ì…‹ë§Œ ê°€ì ¸ì˜´
vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=["<pad>", "<unk>"])
# build_vocab_from_iterator(...): ì „ì²´ í† í° ë¦¬ìŠ¤íŠ¸ë“¤ì„ ëª¨ì•„ ë‹¨ì–´ ì‚¬ì „(vocab) ì„ ìƒì„±
# {"wall": 4, "st": 5, ".": 6, "bears": 7, ...}
# specials=["<pad>", "<unk>"]: íŠ¹ë³„ í† í° ì¶”ê°€, <pad>: ì‹œí€€ìŠ¤ë¥¼ ë™ì¼ ê¸¸ì´ë¡œ ë§ì¶”ê¸° ìœ„í•œ íŒ¨ë”© í† í°, <unk>: ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´(unknown)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í† í°
vocab.set_default_index(vocab["<unk>"])
# vocabì€ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ê°€ ë“¤ì–´ì˜¤ë©´ KeyError ë°œìƒ
# ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ì— ëŒ€í•´ í•­ìƒ <unk> ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •

def text_pipeline(x):
    return vocab(tokenizer(x))
# tokenizer(x): ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í•´ (ì˜ˆ: ["stocks", "are", "rising", "fast", "."])
# vocab(...): ê° ë‹¨ì–´ë¥¼ ì‚¬ì „ì—ì„œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ì˜ˆ: [42, 8, 17, 65, 6])
label_pipeline = lambda x: x - 1  # 0~3 (4ê°œ í´ë˜ìŠ¤)
# AG_NEWSëŠ” 1~4 ë²”ìœ„ì˜ ì •ìˆ˜ ë¼ë²¨ì„ ê°€ì§
# PyTorch ëª¨ë¸ì—ì„œëŠ” ë³´í†µ 0ë¶€í„° ì‹œì‘í•˜ëŠ” í´ë˜ìŠ¤ ë¼ë²¨ì„ ê¸°ëŒ€í•˜ê¸° ë•Œë¬¸ì—, 1 â†’ 0, 2 â†’ 1, ..., 4 â†’ 3 ìœ¼ë¡œ ë³€í™˜

def collate_batch(batch): # ì—¬ëŸ¬ ê°œì˜ ìƒ˜í”Œ(batch)ì„ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ì •ë¦¬(padding í¬í•¨)
    label_list, text_list = [], []
    for label, text in batch:
        label_list.append(torch.tensor(label_pipeline(label), dtype=torch.long))
        # label_pipeline(label): AG_NEWS ë“±ì—ì„œëŠ” ë¼ë²¨ì´ 14 â†’ ì´ë¥¼ 03 ìœ¼ë¡œ ë³€í™˜
        # torch.tensor(..., dtype=torch.long): ë¼ë²¨ì„ LongTensorë¡œ ë³€í™˜ (PyTorch ë¶„ë¥˜ ëª¨ë¸ì˜ ìš”êµ¬ ì‚¬í•­)
        processed_text = torch.tensor(text_pipeline(text), dtype=torch.long)
        # text_pipeline(text): "Stocks are rising" â†’ ["stocks", "are", "rising"] â†’ [14, 23, 7]
        # torch.tensor(...): ì´ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
        # ê²°ê³¼: ê¸¸ì´ê°€ ì„œë¡œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ í…ì„œë“¤ ìƒì„±ë¨
        text_list.append(processed_text)
    return pad_sequence(text_list, batch_first=True, padding_value=vocab["<pad>"]), torch.tensor(label_list)
            # [
            #   [14, 23, 7],
            #   [51, 9],
            #   [3]
            # ]
            # â†’ pad_sequence â†’
            # [
            #   [14, 23, 7],
            #   [51, 9, 0],
            #   [3,  0, 0]
            # ]


# ëª¨ë¸ ì •ì˜
class TextRNN(nn.Module):
    def __init__(self, rnn_type="RNN", vocab_size=20000, embed_dim=100, hidden_dim=100, output_dim=4):
        super().__init__() # ë¶€ëª¨ í´ë˜ìŠ¤ nn.Moduleì˜ ìƒì„±ì í˜¸ì¶œ, PyTorch ëª¨ë¸ì„ ì •ì˜í•  ë•Œ í•­ìƒ í¬í•¨ë˜ì–´ì•¼ í•¨
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab["<pad>"])
        # Embedding ë ˆì´ì–´ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ â†’ ë°€ì§‘ ë²¡í„°(dense vector) ë¡œ ë³€í™˜
        # padding_idx: <pad> í† í°ì€ í•™ìŠµí•˜ì§€ ì•Šë„ë¡ ë§ˆìŠ¤í‚¹
        if rnn_type == "RNN":
            self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)
        elif rnn_type == "LSTM":
            self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        elif rnn_type == "GRU":
            self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)
        else:
            raise ValueError("Unsupported RNN type")
        self.fc = nn.Linear(hidden_dim, output_dim)
        # ìµœì¢…ì ìœ¼ë¡œ RNN/LSTM/GRUì˜ ì¶œë ¥(hidden state)ì„ ë°›ì•„ì„œ ë¶„ë¥˜ ê²°ê³¼ë¡œ ë³€í™˜
        # hidden_dim: ë§ˆì§€ë§‰ hidden stateì˜ ì°¨ì›
        # output_dim: ë¶„ë¥˜ í´ë˜ìŠ¤ ê°œìˆ˜ (AG_NEWSëŠ” 4ê°œ â†’ output_dim=4)


    def forward(self, x):
        x = self.embedding(x) # ê° ì¸ë±ìŠ¤ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        out, _ = self.rnn(x)
        out = out[:, -1, :]
        # ëª¨ë“  ì‹œí€€ìŠ¤ ì¤‘ ë§ˆì§€ë§‰ time step (seq_len-1) ì˜ hidden stateë§Œ ì¶”ì¶œ, 
        # ì¦‰, ê° ë¬¸ì¥ì˜ ìµœì¢… ì˜ë¯¸ ìš”ì•½ ë²¡í„°ë¥¼ ê°€ì ¸ì˜´ â†’ out.shape = [batch_size, hidden_dim]
        return self.fc(out) # ìµœì¢…ì ìœ¼ë¡œ í´ë˜ìŠ¤ë³„ ë¡œì§“(logits) ì¶œë ¥, ë¡œì§“ì€ ì¼ì¢…ì˜ ì ìˆ˜ë¡œ softmax ì „ ê°’

# í•™ìŠµ ë° í‰ê°€
def train_eval(model, train_loader, test_loader, epochs=1):
    device = torch.device("cpu")
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    for _ in range(epochs):
    # "epochs íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•˜ë¼", **"ë°˜ë³µ ë³€ìˆ˜ëŠ” ì•ˆ ì“¸ ê±°ë‹ˆê¹Œ ì´ë¦„ ë¶™ì´ì§€ ë§ˆë¼"**ëŠ” ëœ»ì…ë‹ˆë‹¤.
        model.train()
        for text, label in train_loader:
            text, label = text.to(device), label.to(device)
            optimizer.zero_grad()
            output = model(text)
            loss = criterion(output, label)
            loss.backward()
            optimizer.step()
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
    # ì—­ì „íŒŒ(backpropagation) ì™€ gradient ê³„ì‚° ë¹„í™œì„±í™”
    # í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” í•™ìŠµí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ì™€ ì—°ì‚° íš¨ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©
    # ì´ ë¸”ë¡ ì•ˆì—ì„œëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ

        for text, label in test_loader: # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì˜´
            text, label = text.to(device), label.to(device) # ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ìœ„ì¹˜í•œ ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¹€
            preds = model(text) # ëª¨ë¸ì— ì…ë ¥ì„ ì£¼ê³ , ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ìŒ
            predicted = torch.argmax(preds, dim=1)
            correct += (predicted == label).sum().item()
            total += label.size(0)
    return correct / total

# í•™ìŠµ í•¨ìˆ˜ (trainë§Œ ìˆ˜í–‰)
def train(model, train_loader, epochs=1):
    device = torch.device("cpu")
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    for _ in range(epochs):
        model.train()
        for text, label in train_loader:
            text, label = text.to(device), label.to(device)
            optimizer.zero_grad()
            output = model(text)
            loss = criterion(output, label)
            loss.backward()
            optimizer.step()

# í‰ê°€ í•¨ìˆ˜ (ì •í™•ë„ë§Œ ë°˜í™˜)
def evaluate(model, test_loader):
    device = torch.device("cpu")
    model.to(device)
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for text, label in test_loader:
            text, label = text.to(device), label.to(device)
            preds = model(text)
            predicted = torch.argmax(preds, dim=1)
            correct += (predicted == label).sum().item()
            total += label.size(0)
    return correct / total


# ë°ì´í„° ë¡œë”©
train_iter, test_iter = AG_NEWS(split=('train', 'test'))
train_dataloader = DataLoader(train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(test_iter, batch_size=64, shuffle=False, collate_fn=collate_batch)

continue_training = True  # â† ì¶”ê°€ í•™ìŠµ ì—¬ë¶€ ì„¤ì •
additional_epochs = 1     # â† ì´ì–´ì„œ í•™ìŠµí•  íšŸìˆ˜

# ì„±ëŠ¥ ë¹„êµ
results = {}
for rnn_type in ["RNN", "LSTM", "GRU"]:
    print(f"\n=== {rnn_type} ëª¨ë¸ ì‹œì‘ ===")
    model = TextRNN(rnn_type=rnn_type, vocab_size=len(vocab))

    model_path = f"{rnn_type}_model.pt"

    start = time.time()
    
    if os.path.exists(model_path):
        print("ğŸ” ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ")
        model.load_state_dict(torch.load(model_path))
        
        if continue_training:
            print("â• ì¶”ê°€ í•™ìŠµ ìˆ˜í–‰ ì¤‘...")
            train(model, train_dataloader, epochs=additional_epochs)
            torch.save(model.state_dict(), model_path)
        else:
            print("ğŸ“Š í‰ê°€ë§Œ ìˆ˜í–‰")
        
        acc = evaluate(model, test_dataloader)  # â— í‰ê°€ë§Œ ìˆ˜í–‰
        
    else:
        print("ğŸ†• ëª¨ë¸ í•™ìŠµ ì‹œì‘")        
        train(model, train_dataloader, epochs=1)  # â— í•™ìŠµr)
        torch.save(model.state_dict(), model_path)
        acc = evaluate(model, test_dataloader)    # â— í•™ìŠµ í›„ í‰ê°€        
        results[rnn_type] = {"accuracy": round(acc, 4), "training_time_sec": round(end - start, 2)}

    end = time.time()

    results[rnn_type] = {
        "accuracy": round(acc, 4),
        "training_time_sec": round(end - start, 2)
    }

print("\n=== ì„±ëŠ¥ ë¹„êµ ===")
for model, res in results.items():
    print(f"{model}: Accuracy = {res['accuracy']}, Time = {res['training_time_sec']} sec")
