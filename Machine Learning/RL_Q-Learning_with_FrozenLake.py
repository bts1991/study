import gymnasium as gym
import numpy as np
import random
import time

# í™˜ê²½ ìƒì„±
env = gym.make("FrozenLake-v1", is_slippery=True, render_mode="ansi")

# Q-table ì´ˆê¸°í™”
state_size = env.observation_space.n
action_size = env.action_space.n
q_table = np.zeros((state_size, action_size))

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
alpha = 0.8 ## Q ì—…ë°ì´íŠ¸ ì‹œ, ë¯¸ë˜ Që¥¼ ë°˜ì˜í•  ë¹„ìœ¨
gamma = 0.95 ## í• ì¸ìœ¨, í´ìˆ˜ë¡ ë¯¸ë˜ì˜ Rewardì— ì¶©ì‹¤
epsilon = 1.0 ## í´ìˆ˜ë¡ Random í•˜ê²Œ ì›€ì§ì„, ì²˜ìŒì—ëŠ” ë¬´ì¡°ê±´ Exploration í›„ ì ì  Exploitation
epsilon_min = 0.01
epsilon_decay = 0.995 ## epsilonì— ê³±í•´ì„œ epsilonì„ ì ì  ì‘ì•„ì§€ê²Œ ë§Œë“¦
# episodes = 2000
episodes = 100
## ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ ì‹œì‘í•´ì„œ ì¢…ë£Œ(ëª©í‘œ ë„ë‹¬ ë˜ëŠ” ì‹¤íŒ¨)í•  ë•Œê¹Œì§€ì˜ í•œ ì‚¬ì´í´
## ì—ì´ì „íŠ¸ê°€ ì‹œì‘ ìœ„ì¹˜ Sì—ì„œ ì‹œì‘í•´, ì›€ì§ì´ë©° ëª©ì ì§€ Gë¡œ ê°€ê±°ë‚˜ êµ¬ë© Hì— ë¹ ì§€ë©´ ì—í”¼ì†Œë“œ ì¢…ë£Œ.
## ì´ 2000ë²ˆì˜ í•™ìŠµ ì‚¬ì´í´ì„ ë°˜ë³µí•˜ê² ë‹¤ëŠ” ì˜ë¯¸
total_reward = 0

# í•™ìŠµ ë£¨í”„
for episode in range(episodes):
    state, _ = env.reset()
    done = False

    print("======",f"[Episode {episode}]","=======================================================")
    while not done: ## ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•˜ë‹¤ë©´
        if random.uniform(0, 1) < epsilon: ##  ê· ë“±ë¶„í¬, ì •ê·œë¶„í¬ ì•„ë‹˜
            action = env.action_space.sample() ## ì™¼ìª½(0), ì˜¤ë¥¸ìª½(2), ì•„ë˜(1), ìœ„(3)
            print(f"[íƒí—˜] S:{state}, ë¬´ì‘ìœ„ í–‰ë™ A:{action}")
        else:
            action = np.argmax(q_table[state]) ## ë°°ì—´ ë‚´ ê°€ì¥ í° ì¸ë±ìŠ¤, ì¦‰ ê°€ì¥ ë†’ì€ ë³´ìƒì„ ì„ íƒ
            print(f"[ì´ìš©] S:{state}, ìµœì  í–‰ë™ A:{action}")

        next_state, reward, terminated, truncated, _ = env.step(action)
        total_reward += reward
        done = terminated or truncated
        
        old_value = q_table[state, action]
        next_max = np.max(q_table[next_state])

        ## íŠ¹ì • ìƒíƒœ stateì—ì„œ íŠ¹ì • í–‰ë™ actionì„ í–ˆì„ ë•Œ ì•ìœ¼ë¡œ ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ëˆ„ì  ë³´ìƒì˜ ê¸°ëŒ€ê°’
        q_table[state, action] = q_table[state, action] + alpha * (
            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
        )
 
        print(f"ì´ ëˆ„ì  ë³´ìƒ: {total_reward}")
        # ì—…ë°ì´íŠ¸ ê³¼ì • ì¶œë ¥
        print(f"S:{state}, A:{action} â†’ S':{next_state}, R:{reward}")
        print(f"Q({state},{action}) ì—…ë°ì´íŠ¸: {old_value:.4f} â†’ {q_table[state, action]:.4f}")    

        state = next_state
        

    if epsilon > epsilon_min:
        epsilon *= epsilon_decay

print("âœ… í•™ìŠµ ì™„ë£Œëœ Q-table:")
print(q_table)

# # ------------------------------------------------------
# # ğŸ§­ ì •ì±… ì‹œê°í™” í•¨ìˆ˜
# # ------------------------------------------------------
# def render_policy_from_q_table(q_table, size=4):
#     arrow_dict = {
#         0: 'â†',  # Left
#         1: 'â†“',  # Down
#         2: 'â†’',  # Right
#         3: 'â†‘',  # Up
#     }

#     print("\nğŸ§­ í•™ìŠµëœ ì •ì±… (í™”ì‚´í‘œë¡œ ì‹œê°í™”):\n")
#     for i in range(size):
#         row = ''
#         for j in range(size):
#             state = i * size + j
#             if np.sum(q_table[state]) == 0:
#                 row += 'â–  '  # í•™ìŠµë˜ì§€ ì•Šì€ ìƒíƒœ
#             else:
#                 action = np.argmax(q_table[state])
#                 row += arrow_dict[action] + ' '
#         print(row)

# render_policy_from_q_table(q_table)

# # ------------------------------------------------------
# # ğŸ¬ ìµœì  ì •ì±…ìœ¼ë¡œ ì´ë™ ì‹œë®¬ë ˆì´ì…˜
# # ------------------------------------------------------
# def play_best_policy(q_table, env):
#     state, _ = env.reset()
#     done = False
#     total_reward = 0

#     print("\nğŸ¬ ìµœì  ì •ì±…ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì´ë™:\n")
#     while not done:
#         env.render()
#         action = np.argmax(q_table[state])
#         time.sleep(0.5)
#         next_state, reward, terminated, truncated, _ = env.step(action)
#         done = terminated or truncated
#         total_reward += reward
#         state = next_state

#     print(env.render()) 
#     print(f"\nğŸ ì¢…ë£Œ! ì´ ë³´ìƒ: {total_reward}\n")

# play_best_policy(q_table, env)
