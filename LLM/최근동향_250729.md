# GPT-3's few-shot learning

	사전 훈련된 지식을 바탕으로, 일부 예시(보통 32개)만 보고 작업을 수행
	별도의 파인튜닝 없이 프롬프트만으로 문제 해결이 가능
	
	예시
		Task: “Translate English to French” → 영어를 프랑스어로 번역하라는 작업 설명
		Examples: 모델이 번역을 어떻게 수행해야 하는지 보여주는 문맥과 정답 쌍
			sea otter => loutre de mer
			peppermint => menthe poivrée
			plush giraffe => girafe peluche
		Prompt: 이 예시들과 함께 마지막 줄에 cheese => 만 주어지면, 모델이 이를 보고 “fromage”라는 번역 결과를 생성
		
# Chain-of-Thoughts Prompting

	중간 추론 단계를 거치도록 유도하여 복잡한 추론 문제(complex reasoning) 정확히 수행
	
	중간 추론 단계: 중간 사고 과정(thought chain)을 보여주는 예시(demonstration)를 제공
	
	Standard Prompting (표준 프롬프팅) 예시
		Input: 
			Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
			➜ A: The answer is 11.

			Q: The cafeteria had 23 apples. If they used 20 and bought 6 more, how many apples do they have?
		Output: 
			A: The answer is 27
			
		평가: 틀림. 중간 계산 없이 바로 답을 추측하기 때문에 오류 발생.
		
	Chain-of-Thought Prompting (사고의 흐름 유도 프롬프팅) 예시
		Input: 
			Q: Roger has 5 tennis balls...
			➜ A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.

			Q: The cafeteria had 23 apples...
		
		Output: 
			A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.
			
		평가: 정답 도출에 성공.
		
	단점? 매번 예제를 만들어줘야 함
	도전: 매번 예제를 만들지 않고 바로 Chain-of-Thought을 할 수 없을까?
	
# Zero-shot CoT Prompting

	직접 예제를 주지 않고, 단순히 "A: Let's think step by step" 만 추가해도 성능이 향상됨
	

# Instruction Tuning

	GPT-3 의 단점: 영어는 잘 하는데, 내가 의도한 답을 주지는 않아!
	InstructGPT는 기존 GPT-3를 사용자 지시(instruction)를 따르도록 파인튜닝한 모델
	**인간 피드백을 활용한 강화학습(RLHF: Reinforcement Learning from Human Feedback)**을 통해 개선
	
	Explicit Task Instructions (명시적 작업 지시)
		한 개의 모델로 다중 작업 수행하기 위해
		명령어 기반 방식은 새로운 작업에도 쉽게 일반화 가능
		RL from AI Feedback (RLAIF) 사용: AI가 AI를 평가
		
# Large Multimodal Language Model(LMM)
	
	이미지도 Input으로 받음
	최대 32,768 token을 참고(기존 GPT-3.5는 4,096개만 참고)하여 과거 대화 내용을 더 많이 기억
		-> 더 많은 token을 참고할수록 더 좋은 모델이라고 선전하고 있음
		
# GPT-4O(mni)

	자연스러운 human-computer interaction을 강조
	모든 text, audio, image, video를 조합하여 input으로 사용
		감정도 전달할 수 있음(text+audio)
	text, audio, image를 조합하여 ouput 도출
	
# RAG + GPT-4O

	RAG: Retrieval-Augmented Generation, 검색 증강 생성

	RAG 기술이 LLM의 한계 보완: LLM의 뛰어난 언어 생성 능력과 외부 데이터베이스의 방대한 최신 정보를 결합하여, 훨씬 더 신뢰성 높고 정확한 답변을 제공

# Modality Fusion

	첫 번째 과제: Representation
		텍스트, 이미지 같은 각각의 모달리티를 컴퓨터가 이해할 수 있는 vector로 표현
		
	두 번째 과제: semantic information
		원본 데이터(raw data)가 가진 고유의 **의미 정보(semantic information)**를 잘 담아내야 함께
		
	융합 방법 (Methods)
		Fusion during training (훈련 중 융합): AI 모델을 학습시키는 과정에서부터 융함을 통해 데이터 간의 복잡한 상호 관계를 학습
		
			1. Fusion-free: CLIP 모델
				전용 인코더 사용: 이미지(visual)는 CNN 모델로, 텍스트(text)는 Transformer 모델로 각각 처리(인코딩)
				인코딩된 두 데이터를 합치는 별도의 융합(Fusion) 단계나 모듈이 없다는 것이 큰 특징
				코사인 유사도를 통한 '정렬'이라는 개념을 사용하여 데이터를 합침
					서로 관련된 이미지와 텍스트가 각자의 인코더를 거쳐 벡터로 변환되었을 때, 이 두 벡터가 의미적으로 가까워지도록 학습
			
			2. Late Fusion
				각 데이터를 처리하는 전용 인코더(modality-specific encoders) 뒤에, 두 정보를 융합하기 위한 별도의 '명시적 융합 모듈(explicit fusion module)'이 존재
					즉, 데이터를 단순히 '정렬'하는 것을 넘어, 적극적으로 '결합'하는 단계 추가
				각 인코더에서 나온 결과물(임베딩)은 공유된 잠재 공간(shared latent space)으로 투영된 후, 융합 모듈을 통해 하나의 **결합된 표현(joint representation)**으로 합쳐짐
			
			3. Early Fusion
				서로 다른 모달리티(텍스트, 이미지, 영상 등)의 입력을 하나의 통합된 인코더, 보통 Transformer 구조를 사용하여 함께 처리
					다양한 모달리티의 입력 토큰을 하나의 시퀀스로 결합하여 처리
					Transformer 내부의 self-attention 메커니즘을 공유하여 모달리티 간 상호작용(interaction)을 암묵적으로 학습
		
		Fusion by stitching (연결을 통한 융합): 각 모달리티를 처리하도록 이미 학습된 별도의 모델들을 가져와 '꿰매듯이' 연결하여 융합하는 방식. 모듈처럼 유연하게 모델을 구성할 수 있는 장점.