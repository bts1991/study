# 1장 기초 수학과 미적분

## 오일러의 수와 자연로그
### 1) 오일러의 수
- 오일러의 수 e는 약 2.71828
- 이자를 복리로 계산할 때, 월 -> 일 -> 분 -> 시 -> 무한히 작은 순간을 n에 적용
- $(1+{1 \over n})^n$ 에서 n이 무한대로 커지면 약 2.781828에 수렴
- 오일러의 수가 밑인 지수 함수의 도함수(미분값)가 자기 자신이기 때문에 계산 상의 효율성으로 인해 머신 러닝에서 유용

### 2) 자연로그
- 오일러의 수 e를 밑으로 사용하는 로그: $ln()$
- $ln(10)$은 e를 거듭제곱해서 10이 되는 수수


## 미분(Derivative)
- x의 아주 작은 변화량에 대한 y의 변화량, 기울기
- 편도함수란?
  - $f(x,y) = 2x^3 + 3y^3$ 일 때, x와 y변수는 각각 고유한 도함수(기울기=gradient) $df \over dx$와 $df \over dy$를 갖음
- 연쇄법칙이란?
  - 신경망 층을 훈련할 때 각 노드의 도함수를 곱해서 계산함함
  - 다음 함수 두 개에서 $dz \over dx$를 구하려면?
    - $$y = x^2 + 1, z=y^3-2$$
    - $${dz \over dx} = {dz \over dy}\times{dy \over dx}$$


## 적분

# 2장 확률
## 확률 이해
- 확률(probability) vs 가능도(likelihood)
  - 확률: 일어나지 않은 사건(미래)에 대한 예측
    - Sum(모든 상호 배타적인 결과의 확률) = 1
  - 가능도: 이미 발생한 사건의 빈도를 측정
    - Sum(모든 상호 배타적인 결과의 확률) != 1
  - 머신러닝에서는 확률(미래)을 예측하기 위해 데이터 형태에 대한 가능도(과거)를 사용
  - 오즈(Odds)
    - $$P(X)={O(X) \over {1+O(X)}}$$
    - $$O(X)={P(X) \over {1-P(X)}}$$
    - 오즈가 2.0 이면? 일어날 확률이 그렇지 않을 확률보다 두배 더 높다
    - 백분율보다 더 직관적으로 설명 가능
  - 결합법칙
    - 두 사건이 동시에 일어날 확률
    - 상호 배타적이지 않을 때
      - $P(A \,and\, B) = P(A|B)\times P(B)$
    - 상호 배타적일 때(즉, 독립적일 때)
      - $P(A \,and\, B) = P(A)\times P(B)$
  - 확률의 덧셈 법칙
    - $P(A \,or\, B) = P(A) + P(B) - P(A \,and\, B)$
  - 상호 배타성(독립적)
    - A 또는 B 중 하나의 결과만 가능하고 둘 다 허용 불가
    - $P(A \,and\, B)=0$
## 조건부 확률(Conditional Probability)
- P(A Given B) 또는 P(A|B): B가 발생했을 때, A가 발생할 확률
- 커피가 암에 연관이 있는지 연구하려면, 커피를 마시는 사람이 암에 걸릴 확률(조건부 확률)이 필요
  - P(암|커피)
## 베이즈 정리(Bayesian Rule)
- 새로운 정보가 들어왔을 때, 우리의 믿음(확률)을 어떻게 바궈야 하는지를 알려주는 규칙
- 새로운 증거에 따라 기존 확률(Prior)을 "업데이트(Posteior)"
- $$P(A|B)={P(B|A)\times P(A)\over P(B)} = {P(A \,AND\, B) \over P(B)}$$
- $$사후확률(Posteior) = {{우도(Likelihood)\times 사전확률(Prior)} \over 정규화 상수}$$
## 이항 분포
## 베타 분포

# 3장 기술 통계와 추론 통계

- 기술 통계
  - 데이터를 요약
  - 평균 mean, 중앙값 median, 모드 mode, 차트, 종 곡선을 계산
    - 중앙값: 정렬된 값 집합 중 가장 가운데 값, 이상치로 평균을 신뢰할 수 없을 때 유용한 대안
    - 모드: 가장 자주 발생하는 값
- 추론 통계
  - 표본을 기반으로 모집단에 대한 속성을 발견
  - 일종의 유추

## 3-1 기술 통계
### 모집단
- 연구하고자 하는 특정 관심 그룹
- 분산
  - $$\sigma^2 = {{\sum(x_i-\mu)^2}\over N}$$
- 표준편차
  - $$\sigma=\sqrt {{\sum(x_i-\mu)^2}\over N}$$
### 표본
- 모집단의 하위 집합 
- 무작위, 편향되지 않음
- 모집단에 대한 속성을 추론하기 위한 목적
- 분산
  - $$s^2 = {{\sum(x_i-\bar{x})^2}\over n-1}$$
- 표준편차
  - $$s=\sqrt {{\sum(x_i-\bar{x})^2}\over n-1}$$
- n이 아닌 n-1로 나누는 이유는?
  - 표본의 편향을 줄이고 표본에 기반한 모집단의 분산을 과소평가하지 않기 위해
  - 분모에서 하나 작은 값을 사용함으로써 분산을 증가시키고 표본의 불확실성을 더 많이 포착
### 정규 분포(Normal Distribution), 가우스 분포(Gaussian distribution)
- $N(m, \sigma^2)$
- 평균 근처가 가장 질량이 크고, 대칭 형태를 띤 종 모향 분포
- 자연과 일상생활에서 일어나는 많은 현상이 이 분포를 따르고 있으며
- 어떤 분포라도 표본이 충분이 크면 표본의 평균은 정규 분포를 따름
- 이 분포의 퍼짐 정도 = 표준 편자
- 꼬리는 가능성이 가장 낮은 부분이며 0에 수렴하지만 0은 아님
- 자연과 일상생활에서 일어나는 많은 현상과 유사
- 중심 극한 정리 덕분에 정규 분포가 아닌 문제에도 일반화 가능
- 머신 러닝에서 어떻게 활용 되나?
  - 인공신경망에서 초기 기중치의 값이 정규 분포를 따른다고 가정
  - 각 가중치를 **무작위(random)**로 설정하면 뉴런마다 출력이 다르고, 각기 다른 역할을 하게 되어 학습이 가능(그렇지 않으면, 똑같은 출력을 내고 똑같이 업데이트되기 때문에 학습이 되지 않음)
  - 표준 정규분포를 샘플링 함
### 표준 정규 분포(Standard Normal Distribution)
- 평균이 0이고 표준 편차가 1이 되도록 정규 분포의 크기를 재조정
- 평균과 분산이 다른 정규 분포 간의 퍼짐 정도를 쉽게 비교
- 모든 x 값을 표준 편차, 즉 z 점수로 표현
  - $$z={{x-\mu \over \sigma}}$$
- 변동 계수(Coefficient Variation)
  - 두 분포를 비교해 각 분포가 얼마나 퍼져 있는지 정량화
  - $$cv={\sigma \over \mu}$$

## 3-2. 추론 통계
### 중심 극한 정리(Central Limit Theorem, CLT)
- 모집단에서 충분히 많은 표본을 추출하면 해당 모집단이 정규 분포를 따르지 않더라도 정규 분포가 드러난다?
- 충분히 많은 표본 채취 -> 각각의 평균을 계산 -> 이를 하나의 분포로 표현 -> 정규 분포를 따름
- 특성1: 표본 평균의 평균 = 모집단의 평균
- 특성2: 모집단이 정규 분포이면 표본 평균도 정규 분포
- 특성3: 모집단이 정규 분포가 아니라도 표본 크기가 30보다 큰 경우 표본 평균은 대략적으로 정규 분포를 따름
- 특성4: 표본 평균의 표준 편차는 모집단 표준 편차를 n의 제곱근으로 나눈 값과 동일
  - $$\sigma_{\bar{x}} = {\sigma \over \sqrt n}$$
- So What? 정규 분포가 아닌 경우에도 표본을 기반으로 모집단에 대한 유용한 정보를 추론할 수 있음음
### 신뢰구간
- 표본 평균(또는 파라미터)이 모집단 평균의 특정 범위 속한다고 얼마나 믿는지
- 예시
  - 표본 평균이 64.408이고 표준 편차가 2.05인 골든 리트리버 31마리의 표본을 기준으로 모집단 평균이 63.686에서 65.1296 사이에 있다고 95% 확신한다.
- $\plusmn1.95996$ 은 표준 정규 분포의 중심에서 확률의 95%에 해당하는 임계 z값값
### p 값이란?
### 가설 검정
### t 분포: 소규모 표본 처리


# 4장 선형대수학

## 벡터란
- 벡터는 데이터를 시각적으로 표현
  - 공간상에서 특정 방향과 길이를 가진 화살표로 데이터의 한 조각을 나타냄
    - 따라서, 데이터를 조작하는 것은 곧 벡터를 조작하는 것것
  - 꼬리가 항상 자표 원점(0,0)에서 시작
  - 수평으로 3스텝, 수직으로 2스텝이동하는 벡터는?
    - $$\overrightarrow{v}=\begin{bmatrix} 3 \\ 2 \end{bmatrix}  $$
  - 예시
    - 집의 편적이 18평방미터이고 가치가 30만달러인 데이터
    - $$\overrightarrow{v}=\begin{bmatrix} 18 \\ 30만 \end{bmatrix}  $$
- 벡터의 덧셈에서 교환 법칙(Commutative Property)이 성립
  - $\overrightarrow{v}$이동 후 $\,\overrightarrow{w}$ 이동 하든지, 그 반대로 하든지 상관 없음
- 스케일링
  - 스칼라(Scalar)라는 하나의 값을 곱해 벡터를 늘이거나 줄임
    - 스칼라는 방향은 없고 크기만 있는 수(일반적인 실수나 복소수)
  - 스케일을 조정해도 여전히 같은 선상에 존재함 = 선형종속

## 내적(Dot Product)이란
- 두 벡터가 얼마나 닮았는지를 표현
- $\begin{bmatrix} 1 \\ 3 \end{bmatrix} \cdot \begin{bmatrix} 4 \\ 2 \end{bmatrix}$ = 1 x 4 + 3 x 2 = 10

## Matrix의 4가지 Fundamental Vector Spaces
- 어떤 𝑚×𝑛 행렬 𝐴에 대해, 다음의 네 가지 벡터 공간을 정의
1. 열공간 (Column Space) = $C(A)$
    - Ax의 결과가 나오는 모든 가능한 벡터 공간
    - A가 어떤 벡터 x에 작용해서 도달할 수 있는 영역
2. 영공간 (Null Space) = $N(A)$
    - Ax=0을 만족하는 모든 벡터 x
    - 즉, 행렬 A가 모두 0으로 보내버리는 입력 공간
3. 행공간 (Row Space) = $C(A^T)$
    - A의 행들을 열처럼 생각해서 만든 $A^T$의 열공간
    - A의 행벡터들이 생성하는 공간
4. 좌영공간 (Left Null Space) = $N(A^T)$
    - $ A^Ty=0$ 을 만족하는 모든 벡터
    - 즉, $A^T$가 0으로 보내는 벡터들


## 스팬과 선형 종속
- 다른 두 방향을 향하는 두개의 벡터를 스케일링하고 더하면 세로운 벡터를 얼마든지 만들 수 있음음
- 스팬(Span)
  - 가능한 벡터의 전체 공간
- 선형 독립  
  - 서로 방향이 다른 두 벡터는 선형 독립이며 스팬이 무한함
- 선형 종속
  - 같은 방향이나 선상에 존재하는 두 벡터는 선형 종속
- 선형 독립일 때 벡터들이 유연하게 움직이고, 해를 쉽게 구함
  - 연립 방정식에서 선형 종속이 있는 경우 변수가 사라지기 때문에 문제를 풀 수 없음

## 선형 변환
- 성형 종속의 경우를 제외하고 결합된 벡터는 원하는 방향과 길이를 가질 수 있음
- 기저 벡터의 움직임을 이용해 벡터를 늘이고, 줄이고, 비틀고, 회전
- 선형 변환은 수학적 데이터 조작의 핵심심
- 기저 벡터(Basis Vector)
  - 길이가 1이고 서로 수직이며 양의 방향을 가리킴
  - 모든 벡터를 만들거나 변화하기 위한 구성 요소소
  - $$\hat{i}=\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \,\hat{j}=\begin{bmatrix} 0 \\ 1 \end{bmatrix}, \,기저벡터 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$$
- 4가지 선형변환
  - 스케일: 늘어나거나 줄어듬
  - 회전: 공간을 돌림
  - 반전: 공간을 뒤집어 $\hat{i}$과 $\hat{j}$의 위치를 바꿈
  - 전단: 특정 방향의 직선과의 거리에 비례하여 각 포인트를 이동

## 행력 벡터의 곱셈
- 변환 후에 $\hat{i}$과 $\hat{j}$이 도착하는 곳을 추적하는 개념
- 벡터를 만드는 것과 벡터를 변환하는 것이 실제로 같음을 깨달아야 함
- 기저 벡터가 변환 전후의 출발점이라 생각하면 상대적일 것임임
- 벡터의 곱셈을 통해 $\hat{i}$과 $\hat{j}$이 변할 때 $\overrightarrow{v}$도 같이 변함
- 즉, 행렬의 곱셈은 벡터 공간에 여러 개의 변환을 적용하는 것
  - 순서: 가장 안쪽부터 바깥쪽으로 변환을 적용
  - 행렬A * 행렬B * 행렬C = (행렬A * 행렬B) * 행렬C = 행렬A * (행렬B * 행렬C)

## 행렬식(Determinant)
- 선형 변환을 할 때, 공간을 '확장' 또는 '축소'하게 되는데, 두 벡터로 형성된 영역이 선형 변환에 따라 얼마나 변하는가
  - 2차원 면적, 3차원 이상일 경우 부피
  - 단순 전단이나 회전은 면적이 변하지 않으므로 행렬식에 영향이 없음
- 예시
  - $2\hat{i}$과 $3\hat{j}$
  - 이 변환은 면적을 6배 증가 시킴, $\det(A)$
- 행렬식은 변환이 선형 종속인지 알려줌
  - 행렬식이 0이면, 공간의 면적이 없어지고 더 작은 차원으로 축소되었음을 의미
  - 2차원 -> 1차원, 3차원 -> 2차원
- 공간을 얼마나 쥐어짜거나 늘리는지
  - det=1: 원래 부피 유지
  - det=0: 정보 손실 있음 (데이터가 눌림)
  - ∣det∣>1: 데이터가 확장됨
## 항등 행렬(Identity Matrix)
- 대각선의 값이 1이고 다른 값은 0인 정방 행렬
- $$\begin{bmatrix} 1&0&0 \\ 0&1&0 \\ 0&0&1 \end{bmatrix}$$
- 항등 행렬을 만들 수 있다면 변환을 취소하고 원래 기저 벡터를 찾았다는 의미
## 역행렬(Inverse Matrix)
- 다른 행렬의 변환을 취소 $A^{-1}$
- $A^{-1}$과$\,A$를 곱셈하면 항등 행렬이 됨

# 5장 선형 회귀

# 6장 로지스틱 회귀

# 7장 신경망